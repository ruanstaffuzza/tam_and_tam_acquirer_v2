{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf4517ff-8148-4977-9eb9-2418b9352d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67d9b1f0-40a6-45e9-80a6-34979fbd47fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-31\n",
      "Tratando dados de  2024-08-31\n",
      "[2024-09-26 20:31:01] Getting dataset from BQ...\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando grupos: 100%|██████████| 5571/5571 [00:07<00:00, 787.79grupo/s] \n"
     ]
    }
   ],
   "source": [
    "from main_agrupamento_nomes import main_agrupamento_nomes\n",
    "main_agrupamento_nomes('2024-08-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57bd5cbf-2d94-4b14-abcd-06a41cef5ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/jupyter/Projects/TAM_Ruan', '/opt/conda/lib/python37.zip', '/opt/conda/lib/python3.7', '/opt/conda/lib/python3.7/lib-dynload', '', '/home/jupyter/.local/lib/python3.7/site-packages', '/opt/conda/lib/python3.7/site-packages', '/opt/conda/lib/python3.7/site-packages/IPython/extensions', '/home/jupyter/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Verify it has been added\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89bdcd4-5c49-4bf9-8001-8632c531edad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "from projectutils import read_data \n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7cb0f96-de5a-464c-a8eb-ca8bfd07207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys, importlib\n",
    "#importlib.reload(sys.modules['group_tam_id'])\n",
    "from group_tam_id import (\n",
    "    assign_group_ids,\n",
    "    open_ton, load_to_gbq, create_doc_final, open_init_data,\n",
    "    execute_with_context,\n",
    "    init_group_id, \n",
    "    deal_merged_places, \n",
    "    deal_merged_docs,\n",
    "    deal_unmerged_places, \n",
    "    choose_prefered_document, \n",
    "    create_agrupamento_inspecao, \n",
    "    grouped_subs_asterisk, \n",
    "    final_ajustes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ba22f3-c4c0-46f3-8aa4-df5caded09bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_main_data_abr():\n",
    "\n",
    "    df = read_data('final_nomes')\n",
    "    df = df[df['reference_month']==np.datetime64('2024-04-30T00:00:00.000000000')]\n",
    "\n",
    "    LEVEL_GROUP = ['cod_muni', 'reference_month']\n",
    "    OUTROS = ['Outros', 'Outros_Pags', 'Outros_SumUp', 'Outros_Stone']\n",
    "\n",
    "    print('Apenas mmhid de Outro')\n",
    "    df['merchant_market_hierarchy_id'] = df['merchant_market_hierarchy_id'].where(df['subs_asterisk'].isin(OUTROS), None)\n",
    "\n",
    "\n",
    "    df = df.drop_duplicates(['subs_asterisk', 'nome_master', 'merchant_market_hierarchy_id'] + LEVEL_GROUP)\n",
    "\n",
    "    ton = open_ton(read_data, LEVEL_GROUP)\n",
    "    ton = ton[ton['reference_month']==np.datetime64('2024-04-30T00:00:00.000000000')]\n",
    "\n",
    "    df = pd.concat([\n",
    "            df,\n",
    "            ton,\n",
    "        ])\n",
    "    df['nome_master'] = df['nome_master'].str.replace(r'[^A-Z]', '', regex=True)\n",
    "    df['inicio'] = df['nome_master'].str[:6]\n",
    "\n",
    "    df['cod_muni'] = df['cod_muni'].fillna(9999)\n",
    "        #df_orig = df_orig[~df_orig['cod_muni'].isin(saved_munis)]\n",
    "    df['nome_muni'] = df['nome_muni'].fillna('Desconhecido')\n",
    "    df['uf'] = df['uf'].fillna('DE')\n",
    "\n",
    "    df.to_parquet('data/main_data_2024-04-30.parquet')\n",
    "\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def create_main_data(input_date: str):\n",
    "\n",
    "    # Convert input date string to datetime object\n",
    "    reference_date = np.datetime64(datetime.strptime(input_date, '%Y-%m-%d'))\n",
    "\n",
    "    df = read_data('final_nomes')\n",
    "    df = df[df['reference_month'] == reference_date]\n",
    "\n",
    "    LEVEL_GROUP = ['cod_muni', 'reference_month']\n",
    "    OUTROS = ['Outros', 'Outros_Pags', 'Outros_SumUp', 'Outros_Stone']\n",
    "\n",
    "    print('Apenas mmhid de Outro')\n",
    "    df['merchant_market_hierarchy_id'] = df['merchant_market_hierarchy_id'].where(df['subs_asterisk'].isin(OUTROS), None)\n",
    "\n",
    "    df = df.drop_duplicates(['subs_asterisk', 'nome_master', 'merchant_market_hierarchy_id'] + LEVEL_GROUP)\n",
    "\n",
    "    ton = open_ton(read_data, LEVEL_GROUP)\n",
    "    ton = ton[ton['reference_month'] == reference_date]\n",
    "\n",
    "    df = pd.concat([df, ton])\n",
    "    df['nome_master'] = df['nome_master'].str.replace(r'[^A-Z]', '', regex=True)\n",
    "    df['inicio'] = df['nome_master'].str[:6]\n",
    "\n",
    "    df['cod_muni'] = df['cod_muni'].fillna(9999)\n",
    "    df['nome_muni'] = df['nome_muni'].fillna('Desconhecido')\n",
    "    df['uf'] = df['uf'].fillna('DE')\n",
    "\n",
    "    # Save to parquet file with the input date in the filename\n",
    "    df.to_parquet(f'data/main_data_{input_date}.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78cdfe9f-aca9-4009-84b4-423aa4f98d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_read_data():\n",
    "    df_orig = read_data('final_nomes')\n",
    "    LEVEL_GROUP = ['cod_muni', 'reference_month']\n",
    "    ton_orig = open_ton(read_data, LEVEL_GROUP)\n",
    "    df_orig = open_init_data(df_orig, ton_orig, LEVEL_GROUP)\n",
    "    del ton_orig\n",
    "    \n",
    "    #saved_munis = find_saved_munis()\n",
    "    df_orig['cod_muni'] = df_orig['cod_muni'].fillna(9999)\n",
    "    #df_orig = df_orig[~df_orig['cod_muni'].isin(saved_munis)]\n",
    "    df_orig['nome_muni'] = df_orig['nome_muni'].fillna('Desconhecido')\n",
    "    df_orig['uf'] = df_orig['uf'].fillna('DE')\n",
    "    return df_orig\n",
    "\n",
    "def save_main_data():\n",
    "    df = main_read_data()\n",
    "    df.to_parquet('data/main_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba9be2c9-71df-4326-86f1-ac02317a393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def group_and_clean(df, LEVEL_GROUP):\n",
    "    df = df.copy()\n",
    "    from test_grouping import group_names_df\n",
    "\n",
    "    print('Tamanho do DataFrame:', df.shape)\n",
    "\n",
    "    #df['qtd_por_inicio'] = df.groupby(['nome_muni', 'uf'])['nome_master'].transform('count')\n",
    "    #df = df[df['qtd_por_inicio']>1]\n",
    "\n",
    "    df = df.sort_values(by=LEVEL_GROUP + ['nome_master']).reset_index(drop=True)\n",
    "\n",
    "    df = group_names_df(df, group_cols=[\n",
    "                                        'inicio',\n",
    "                                        ] + LEVEL_GROUP, \n",
    "                        nome_col='nome_master',\n",
    "                        delete_aux_cols=False,\n",
    "                        verbose=False)\n",
    "    \n",
    "    if LEVEL_GROUP:\n",
    "        df['agrupamento_nome_1'] = df['res_aux'].apply(lambda x: ', '.join(x)) + df[LEVEL_GROUP].astype(str).apply(lambda x: '|'.join(x), axis=1)\n",
    "    else:\n",
    "        df['agrupamento_nome_1'] = df['res_aux'].apply(lambda x: ', '.join(x))\n",
    "    \n",
    "    df = df.drop(columns=['resultado_prefixes', 'res_aux', 'retirado_de_resultado'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_name_grouping(df, cod_muni, output_dir):\n",
    "    print('-----------\\n Analyzing:', cod_muni)\n",
    "    df = df.copy()\n",
    "    LEVEL_GROUP = []\n",
    "    df = group_and_clean(df, LEVEL_GROUP)\n",
    "    df.to_parquet(f\"{output_dir}/part_{cod_muni}.parquet\")\n",
    "    print('Saved:', cod_muni)\n",
    "\n",
    "def vm_save_grouping(read_data_func):\n",
    "    output_dir = r'data/nomes_agrupados/2024-04-30'\n",
    "    df = read_data(f'main_data_2024-04-30')\n",
    "    LEVEL_GROUP = ['cod_muni', 'reference_month']\n",
    "\n",
    "    df = df[['nome_master', 'inicio', 'cod_muni', 'reference_month']].drop_duplicates()\n",
    "    unique_cod_muni = df['cod_muni'].unique()\n",
    "    for muni in unique_cod_muni:\n",
    "        save_name_grouping(df[df['cod_muni']==muni], muni, output_dir)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cfe58f-b8c7-4aa0-937c-70ab2c77a0a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#vm_save_grouping(read_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1288ba6-8228-470e-99a1-cba23cf816f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vm_save_grouping(read_data_func, input_date):\n",
    "    output_dir = f'data/nomes_agrupados/{input_date}'\n",
    "    df = read_data(f'main_data_{input_date}')\n",
    "    LEVEL_GROUP = ['cod_muni', 'reference_month']\n",
    "\n",
    "    df = df[['nome_master', 'inicio', 'cod_muni', 'reference_month']].drop_duplicates()\n",
    "    unique_cod_muni = df['cod_muni'].unique()\n",
    "    for muni in unique_cod_muni:\n",
    "        save_name_grouping(df[df['cod_muni']==muni], muni, output_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "640c9c47-39b2-419b-b29c-a3b4f051d3ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#vm_save_grouping(read_data, '2024-03-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "597ceb92-d042-4e28-b161-a5c53d0d9640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apenas mmhid de Outro\n"
     ]
    }
   ],
   "source": [
    "def main(date):\n",
    "    create_main_data(date)\n",
    "    vm_save_grouping(read_data, date)\n",
    "    \n",
    "create_main_data('2024-08-31')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e10beff2-ed99-4791-bf77-d140f90ca4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vm_save_grouping(read_data, '2024-08-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "252fe0b7-9ab0-4185-a32f-fc05f82960d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#vm_save_grouping(read_data, '2024-04-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6165d8d6-d2e3-44b2-8b55-61924ef34ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def vm_load_grouping():\n",
    "    import os\n",
    "    start_time = time.time()\n",
    "    output_dir = r'data/nomes_agrupados'\n",
    "    df = pd.concat([pd.read_parquet(f\"{output_dir}/{f}\") for f in os.listdir(output_dir)])\n",
    "    print(f'Loading data in {time.time()  - start_time:.2f} seconds.')\n",
    "    df['agrupamento_nome_1'] = df['agrupamento_nome_1'] + df['cod_muni'].astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "def vm_load_grouping_part(input_date, cod_munis):\n",
    "    import os\n",
    "    start_time = time.time()\n",
    "    output_dir = fr'data/nomes_agrupados/{input_date}' \n",
    "    df = pd.concat([pd.read_parquet(f\"{output_dir}/part_{cod_muni}.parquet\") for cod_muni in cod_munis])\n",
    "    print(f'Loading data in {time.time()  - start_time:.2f} seconds.')\n",
    "    df['agrupamento_nome_1'] = df['agrupamento_nome_1'] + df['cod_muni'].astype(str)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df7e9764-2a85-4a95-9f95-253067e7cab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_grouped(df, LEVEL_GROUP):\n",
    "    df = df.copy()\n",
    "    print('Tamanho do DataFrame:', df.shape)\n",
    "    df['cod_muni'] = df['cod_muni'].fillna(9999)\n",
    "    \n",
    "    df = df.merge(vm_load_grouping(), on=['inicio', 'cod_muni', 'nome_master'], how='inner', indicator=True)\n",
    "    display(df[df['_merge']!='both'])\n",
    "    return df\n",
    "\n",
    "def add_grouped_part(df, input_date, cod_munis):\n",
    "    df = df.copy()\n",
    "    print('Tamanho do DataFrame:', df.shape)\n",
    "    df = df.merge(vm_load_grouping_part(input_date, cod_munis), on=['inicio', 'cod_muni', 'nome_master'], how='inner', indicator=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c12aa6c-fff9-4036-a2e6-8dac872c70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OUTROS = ['Outros', 'Outros_Pags', 'Outros_SumUp', 'Outros_Stone']\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df344c44-cf3c-48fe-963c-6c04edf06bf1",
   "metadata": {},
   "source": [
    "def find_saved_munis():\n",
    "    output_dir = r'data/tratados'\n",
    "    #df = pd.concat([pd.read_parquet(f\"{output_dir}/{f}\", columns=['cod_muni']).drop_duplicates() for f in os.listdir(output_dir)])\n",
    "    df = pd.concat([pd.read_parquet(f\"{output_dir}/{f}\")for f in os.listdir(output_dir)])\n",
    "    #munis = df['cod_muni'].tolist()\n",
    "    return df #munis\n",
    "\n",
    "#df = find_saved_munis()\n",
    "#del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9edde313-3035-456e-946f-6abff5e53d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_read_data():\n",
    "    df_orig = read_data('final_nomes')\n",
    "    LEVEL_GROUP = ['cod_muni']\n",
    "    ton_orig = open_ton(read_data, LEVEL_GROUP)\n",
    "    df_orig = open_init_data(df_orig, ton_orig, LEVEL_GROUP)\n",
    "    del ton_orig\n",
    "    \n",
    "    #saved_munis = find_saved_munis()\n",
    "    df_orig['cod_muni'] = df_orig['cod_muni'].fillna(9999)\n",
    "    #df_orig = df_orig[~df_orig['cod_muni'].isin(saved_munis)]\n",
    "    df_orig['nome_muni'] = df_orig['nome_muni'].fillna('Desconhecido')\n",
    "    df_orig['uf'] = df_orig['uf'].fillna('DE')\n",
    "    return df_orig\n",
    "\n",
    "def save_main_data():\n",
    "    df = main_read_data()\n",
    "    df.to_parquet('data/main_data.parquet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1225db4-58ab-4d94-98de-86621a4821ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_doc_final(df):\n",
    "\n",
    "    df['qtd_mmhid_tax_id'] = df.groupby('merchant_tax_id')['merchant_market_hierarchy_id'].transform('nunique')\n",
    "    df['qtd_mmhid_tax_id'] = df.groupby('tam_id')['qtd_mmhid_tax_id'].transform('max')\n",
    "\n",
    "\n",
    "    df = assign_group_ids(df, ['merchant_tax_id', 'tam_id'], final_col='grouped_tax_id')\n",
    "\n",
    "    df['qtd_nomes'] = df.groupby('group_id')['nome_master'].transform('nunique')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05694ec9-763d-456d-8ded-8ff599d5cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_data_treat_muni(df, file_name):\n",
    "\n",
    "    \"\"\"\n",
    "    Main function to process data with options to display intermediate DataFrames and print verbose messages.\n",
    "\n",
    "    Parameters:\n",
    "    - read_data_func: Function to read initial data.\n",
    "    - display_flag: Boolean flag indicating whether to display the DataFrame after each processing step.\n",
    "    - verbose: Boolean flag indicating whether to print verbose messages during processing.\n",
    "    \"\"\"\n",
    "    display_flag=False\n",
    "    \n",
    "    len_df = len(df)\n",
    "    \n",
    "    #if len_df> 100000:\n",
    "    #    verbose = True\n",
    "    #else:\n",
    "    #    verbose = False\n",
    "    verbose = True\n",
    "    LEVEL_GROUP = ['cod_muni']\n",
    "    \n",
    "    \n",
    "    print(f'Starting main data treat for file {file_name}, with size {len(df)}')\n",
    "\n",
    "    start = time.time()\n",
    "    execute_ctx = execute_with_context(LEVEL_GROUP, display_flag=display_flag, verbose=verbose)\n",
    "\n",
    "    df = execute_ctx(init_group_id, df)\n",
    "    df = execute_ctx(deal_unmerged_places, df)\n",
    "    df = execute_ctx(deal_merged_places, df)\n",
    "    df = execute_ctx(deal_merged_docs, df)\n",
    "    df = execute_ctx(choose_prefered_document, df)\n",
    "    df = execute_ctx(create_agrupamento_inspecao, df)\n",
    "    df = execute_ctx(grouped_subs_asterisk, df)\n",
    "    df = execute_ctx(final_ajustes, df)\n",
    "    df = execute_ctx(create_doc_final, df)\n",
    "    \n",
    "    print(f'Main data treat Execution time file {file_name}: {time.time() - start:.2f} seconds')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95896bcf-8201-49af-ae76-845f27e0af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inter_data_treat_muni(df, file_idx):\n",
    "\n",
    "    len_df = len(df)\n",
    "    \n",
    "    #if len_df> 100000:\n",
    "    #    verbose = True\n",
    "    #else:\n",
    "    #    verbose = False\n",
    "    verbose = True\n",
    "    LEVEL_GROUP = ['cod_muni']\n",
    "    \n",
    "    cod_munis = list(df['cod_muni'].unique())\n",
    "    \n",
    "    start = time.time()\n",
    "    if len(cod_munis) > 3:\n",
    "        print(f'Starting inter data treat for qtd munis {len(cod_munis)}, with size {len(df)}')\n",
    "    else:\n",
    "        print(f'Starting inter data treat for munis {cod_munis}, with size {len(df)}')\n",
    "        \n",
    "\n",
    "    df = add_grouped_part(df, cod_munis) \n",
    "   \n",
    "    print(f'Inter data treat Execution time file {file_idx}: {time.time() - start:.2f} seconds')\n",
    "    return df\n",
    "\n",
    "def save_intermediary(df, idx):\n",
    "    df = inter_data_treat_muni(df, idx)\n",
    "    df.to_parquet(f'data/intermediary/part_{idx}.parquet')\n",
    "    print(f'File part_{idx}.parquet Saved')\n",
    "                  \n",
    "\n",
    "def save_tratados(file_name, input_date):\n",
    "    df = pd.read_parquet(f'data/intermediary/{input_date}/{file_name}')\n",
    "    df = main_data_treat_muni(df, file_name)\n",
    "    df.to_parquet(f'data/tratados/{input_date}/{file_name}')\n",
    "    print(f'File {file_name} Saved in Tratados')\n",
    "              \n",
    "def save_all_tratados(input_date):\n",
    "    output_dir = f'data/intermediary/{input_date}'\n",
    "    files = [x for x in os.listdir(output_dir) if x.endswith('parquet')]\n",
    "    for f in files:\n",
    "        save_tratados(f, input_date)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "def accumulate_and_apply_v0():\n",
    "\n",
    "    \"\"\"\n",
    "    Accumulates rows of a DataFrame based on a unique identifier and applies a function \n",
    "    to the accumulated DataFrame whenever a minimum row threshold is reached.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    func (function): The function to apply to the accumulated DataFrame.\n",
    "    id_column (str): The name of the column with unique identifiers.\n",
    "    min_rows_per_file (int): The minimum number of rows required before applying the function.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    func = save_intermediary\n",
    "    min_rows_per_file = 500000\n",
    "    id_column = 'cod_muni'\n",
    "    \n",
    "    # Initialize the current file index and the DataFrame to accumulate rows\n",
    "    current_file_index = 39\n",
    "    accumulated_df = pd.DataFrame()\n",
    "    len_accu = 0 \n",
    "    \n",
    "    \n",
    "    #df = main_read_data()\n",
    "    df = read_data('main_data')\n",
    "    # Loop through each unique value in the identifier column\n",
    "    \n",
    "    print('Main Data Loaded')\n",
    "    display(df)\n",
    "    print(df.columns)\n",
    "    \n",
    "    \n",
    "    def find_saved_munis():\n",
    "        output_dir = r'data/intermediary'\n",
    "        df = pd.concat([pd.read_parquet(f\"{output_dir}/{f}\", columns=['cod_muni']).drop_duplicates() for f in os.listdir(output_dir)])\n",
    "        saved_munis = df['cod_muni'].tolist()\n",
    "        return saved_munis\n",
    "    \n",
    "    df = df[~df['cod_muni'].isin(find_saved_munis())] \n",
    "    \n",
    "    \n",
    "    \n",
    "    for unique_id in df[id_column].unique():\n",
    "        # Subset the DataFrame for the current unique identifier\n",
    "        subset_df = df[df[id_column] == unique_id]\n",
    "        print(f'Filter {unique_id}')\n",
    "        df = df[df[id_column] != unique_id]\n",
    "        \n",
    "        len_subset_df = len(subset_df)\n",
    "\n",
    "        # Check if the accumulated DataFrame and the subset exceed the minimum row threshold\n",
    "        if len_accu + len_subset_df > min_rows_per_file:\n",
    "            # Apply the function to the accumulated DataFrame\n",
    "            func(accumulated_df, current_file_index)\n",
    "            # Increment the file index and reset the accumulated DataFrame\n",
    "            current_file_index += 1\n",
    "            accumulated_df = subset_df\n",
    "            len_accu = len(accumulated_df)\n",
    "        else:\n",
    "            # Concatenate the subset DataFrame to the accumulated DataFrame\n",
    "            accumulated_df = pd.concat([accumulated_df, subset_df], ignore_index=True)\n",
    "            len_accu = len(accumulated_df)\n",
    "            \n",
    "            \n",
    "    \n",
    "    # Apply the function to any remaining rows in the accumulated DataFrame\n",
    "    if not accumulated_df.empty:\n",
    "        func(accumulated_df, current_file_index)\n",
    "        \n",
    "def accumulate_and_apply_v2(func):\n",
    "    \"\"\"concat files in 'nomes_agrupados', based in 'dict_files' and apply function\"\"\"\n",
    "\n",
    "    dict_files = create_agg_nomes_agrupados()\n",
    "\n",
    "    df = read_data('main_data')\n",
    "\n",
    "\n",
    "    len_df = len(df)\n",
    "    verbose = True\n",
    "    LEVEL_GROUP = ['cod_muni']\n",
    "    \n",
    "    execute_ctx = execute_with_context(LEVEL_GROUP, display_flag=display_flag, verbose=verbose)\n",
    "\n",
    "    \n",
    "    start = time.time()\n",
    "    if len(cod_munis) > 3:\n",
    "        print(f'Starting inter data treat for qtd munis {len(cod_munis)}, with size {len(df)}')\n",
    "    else:\n",
    "        print(f'Starting inter data treat for munis {cod_munis}, with size {len(df)}')\n",
    "    \n",
    "    #df = add_grouped_part(df, cod_munis) \n",
    "    df = execute_ctx(add_grouped, df)\n",
    "\n",
    "    print(f'Inter data treat Execution time file {file_idx}: {time.time() - start:.2f} seconds')\n",
    "\n",
    "    list_cod_munis = [v for k, v in dict_files.items()]\n",
    "    dfs = [df[df['cod_muni'].isin(cod_munis)] for cod_munis in list_cod_munis]\n",
    "\n",
    "    for df in dfs:\n",
    "        df.to_parquet('data/nomes_agrupados/df.parquet', index=False)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f72fb130-aae9-416b-ba7f-16255fb07f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def generate_parquet_summary(directory):\n",
    "    # Lista todos os arquivos no diretório\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.parquet')]\n",
    "    \n",
    "    # Cria listas para armazenar os nomes dos arquivos e a contagem de linhas\n",
    "    file_names = []\n",
    "    line_counts = []\n",
    "    \n",
    "    # Itera sobre cada arquivo, conta as linhas e armazena as informações\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        file_names.append(file)\n",
    "        line_counts.append(parquet_file.metadata.num_rows)\n",
    "    \n",
    "    # Cria o DataFrame\n",
    "    summary_df = pd.DataFrame({\n",
    "        'file_name': file_names,\n",
    "        'line_count': line_counts\n",
    "    })\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "\n",
    "def calculate_group_ids(column):\n",
    "    # Inicializa listas para soma cumulativa e IDs\n",
    "    stopped_cumsum = []\n",
    "    group_ids = []\n",
    "    \n",
    "    # Variáveis auxiliares\n",
    "    cumsum = 0\n",
    "    group_id = 1\n",
    "    \n",
    "    # Itera pelos valores da coluna\n",
    "    for value in column:\n",
    "        if cumsum + value > 1000000:\n",
    "            group_id += 1\n",
    "            cumsum = 0\n",
    "        \n",
    "        cumsum += value\n",
    "        stopped_cumsum.append(cumsum)\n",
    "        group_ids.append(group_id)\n",
    "        \n",
    "        if cumsum == 5:\n",
    "            cumsum = 0\n",
    "            group_id += 1\n",
    "    \n",
    "    return pd.Series(group_ids, index=column.index)\n",
    "\n",
    "\n",
    "def create_agg_nomes_agrupados(input_date):\n",
    "    # Exemplo de uso\n",
    "    directory = f'data/nomes_agrupados/{input_date}'\n",
    "    df = generate_parquet_summary(directory)\n",
    "    df = df.sort_values('line_count')\n",
    "    df['id'] = calculate_group_ids(df['line_count'])\n",
    "    df['cod_muni'] = df['file_name'].apply(lambda filename: filename.split('_')[1].split('.')[0])\n",
    "\n",
    "    import json\n",
    "    dict_files = df.groupby('id')['cod_muni'].apply(lambda x: list(x)).to_dict()\n",
    "    return dict_files\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f002839-55ce-4d59-aefe-9baea0b53001",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "def accumulate_and_apply(input_date):\n",
    "    \n",
    "    def save_func(df, idx, cod_munis, input_date):\n",
    "        print(f'Starting inter data treat for file {idx}')\n",
    "        start = time.time()\n",
    "        df = add_grouped_part(df, input_date, cod_munis)\n",
    "        df.to_parquet(f'data/intermediary/{input_date}/part_{idx}.parquet')\n",
    "        print(f'Inter data treat Execution time: {time.time() - start:.2f} seconds')\n",
    "        print(f'File part_{idx}.parquet Saved') \n",
    "    \n",
    "    \"\"\"concat files in 'nomes_agrupados', based in 'dict_files' and apply function\"\"\"\n",
    "\n",
    "    dict_files = create_agg_nomes_agrupados(input_date)\n",
    "    dict_files = {k: [int(x) for x in v] for k, v in dict_files.items()}\n",
    "\n",
    "    df = read_data(f'main_data_{input_date}')\n",
    "    print(f'File Read: {input_date}')\n",
    "    [save_func(df[df['cod_muni'].isin(v)], k, v, input_date) for k, v in dict_files.items()]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2f73de6-5170-4ab7-a375-4357f82250be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accumulate_and_apply('2024-04-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d7657bc-976d-4257-b8cb-b7fb16bd50b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext line_profiler\n",
    "#%lprun -f deal_merged_places deal_merged_places(df, LEVEL_GROUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d6273e9-49f6-4f06-a56c-3020332036cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, importlib\n",
    "importlib.reload(sys.modules['group_tam_id'])\n",
    "from group_tam_id import (\n",
    "    assign_group_ids,\n",
    "    open_ton, load_to_gbq, create_doc_final, open_init_data,\n",
    "    execute_with_context,\n",
    "    init_group_id, \n",
    "    deal_merged_places, \n",
    "    deal_merged_docs,\n",
    "    deal_unmerged_places, \n",
    "    choose_prefered_document, \n",
    "    create_agrupamento_inspecao, \n",
    "    grouped_subs_asterisk, \n",
    "    final_ajustes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84bb212e-5fd9-4ef3-ba24-cf2a46caa902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_all_tratados('2024-04-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f93ebc09-a55c-428a-807b-e5ba7a9fd163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fim\n"
     ]
    }
   ],
   "source": [
    "print('Fim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d3acb86-7cad-49be-b219-b2f436e2d684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "def upload_partitioned_table_to_bigquery(df, destination_table):\n",
    "    \"\"\"\n",
    "    Uploads a DataFrame to BigQuery with partitioning on 'ingestion_date' column.\n",
    "    If the table exists, it appends the data. If not, it creates the table.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to upload.\n",
    "        destination_table (str): Destination table in the format 'project.dataset.table'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Add 'ingestion_date' column\n",
    "    df['ingestion_date'] = datetime.now()\n",
    "\n",
    "    # Create BigQuery client\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Define job configuration\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        time_partitioning=bigquery.TimePartitioning(\n",
    "            type_=bigquery.TimePartitioningType.DAY,\n",
    "            field=\"ingestion_date\"\n",
    "        ),\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND  # Append data if table exists, otherwise create table\n",
    "    )\n",
    "\n",
    "    # Load DataFrame to BigQuery\n",
    "    job = client.load_table_from_dataframe(df, destination_table, job_config=job_config)\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    job.result()\n",
    "    print(\"Data successfully loaded into BigQuery\")\n",
    "    \n",
    "    \n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_available_data(directory, destination_table, final_data_treat_func, track_file='uploaded_files.json', restart_track_file=True):\n",
    "    \"\"\"\n",
    "    Loads data from parquet files in the given directory and uploads them to a BigQuery table.\n",
    "    Tracks uploaded files to avoid re-uploading.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The directory containing parquet files.\n",
    "        destination_table (str): The BigQuery table to upload data to.\n",
    "        track_file (str): The path to the JSON file used to track uploaded files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the list of uploaded files\n",
    "    if os.path.exists(track_file) and not restart_track_file:\n",
    "        with open(track_file, 'r') as f:\n",
    "            uploaded_files = json.load(f)\n",
    "    else:\n",
    "        uploaded_files = []\n",
    "\n",
    "    # Get list of files in the directory\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('parquet')]\n",
    "\n",
    "    for file in files:\n",
    "        if file not in uploaded_files:\n",
    "            print(f'Opening {file}...')\n",
    "            df = pd.read_parquet(os.path.join(directory, file))\n",
    "            \n",
    "            df = final_data_treat_func(df)\n",
    "            \n",
    "            print(f'Sending {file} to {destination_table}')\n",
    "            upload_partitioned_table_to_bigquery(df, destination_table)\n",
    "            \n",
    "            print(f'{file} sent to {destination_table} successfully!')\n",
    "\n",
    "            # Update the list of uploaded files and save it\n",
    "            uploaded_files.append(file)\n",
    "            with open(track_file, 'w') as f:\n",
    "                json.dump(uploaded_files, f)\n",
    "        else:\n",
    "            print(f'{file} has already been uploaded. Skipping.')\n",
    "\n",
    "\n",
    "def final_data_treat(df):\n",
    "    if '_merge' in df.columns:\n",
    "        df = df.drop(columns=['_merge'])             \n",
    "    else:\n",
    "        print('_merge not in df')\n",
    "    \n",
    "    if 'reference_month_x' in df.columns:\n",
    "        df['reference_month'] = df['reference_month_x']\n",
    "        del df['reference_month_x'], df['reference_month_y']\n",
    "             \n",
    "    #df['group_id_index'] = (df['group_id_index'].astype(str) + df['cod_muni'].astype(str)).astype(int)\n",
    "    \n",
    "    #df = df.drop(columns = ['inicio'])\n",
    "             \n",
    "    return df\n",
    "\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c8bfd4f-04ea-4216-92ec-d8eff9ad73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_available_data(directory='data/tratados/2024-04-30', final_data_treat_func=final_data_treat, destination_table='dataplatform-prd.master_contact.temp_nomes_agrupados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8e5ffce-d118-4a1e-832c-719d49289bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fim Upload\n"
     ]
    }
   ],
   "source": [
    "print('Fim Upload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ece7647-5cca-4686-8a37-4bf002020cb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1519832948.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/tmp/ipykernel_29344/1519832948.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def open_all_intermediary(directory)\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def open_all_intermediary(directory)\n",
    "    \"\"\"\n",
    "    Loads data from parquet files in the given directory and uploads them to a BigQuery table.\n",
    "    Tracks uploaded files to avoid re-uploading.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The directory containing parquet files.\n",
    "        destination_table (str): The BigQuery table to upload data to.\n",
    "        track_file (str): The path to the JSON file used to track uploaded files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the list of uploaded files\n",
    "    if os.path.exists(track_file):\n",
    "        with open(track_file, 'r') as f:\n",
    "            uploaded_files = json.load(f)\n",
    "    else:\n",
    "        uploaded_files = []\n",
    "\n",
    "    # Get list of files in the directory\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('parquet')]\n",
    "\n",
    "    for file in files:\n",
    "        if file not in uploaded_files:\n",
    "            print(f'Opening {file}...')\n",
    "            df = pd.read_parquet(os.path.join(directory, file))\n",
    "            \n",
    "            df = final_data_treat_func(df)\n",
    "            \n",
    "            print(f'Sending {file} to {destination_table}')\n",
    "            upload_partitioned_table_to_bigquery(df, destination_table)\n",
    "            \n",
    "            print(f'{file} sent to {destination_table} successfully!')\n",
    "\n",
    "            # Update the list of uploaded files and save it\n",
    "            uploaded_files.append(file)\n",
    "            with open(track_file, 'w') as f:\n",
    "                json.dump(uploaded_files, f)\n",
    "        else:\n",
    "            print(f'{file} has already been uploaded. Skipping.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5edebdf5-a585-4db6-9f85-987510cf9720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(df):\n",
    "    display_flag=False\n",
    "\n",
    "    len_df = len(df)\n",
    "\n",
    "    verbose = True\n",
    "    LEVEL_GROUP = ['cod_muni']\n",
    "\n",
    "    cod_munis = list(df['cod_muni'].unique())\n",
    "\n",
    "\n",
    "    if len(cod_munis) > 3:\n",
    "        print(f'Starting main data treat for qtd munis {len(cod_munis)}, with size {len(df)}')\n",
    "    else:\n",
    "        print(f'Starting main data treat for munis {cod_munis}, with size {len(df)}')\n",
    "\n",
    "    start = time.time()\n",
    "    execute_ctx = execute_with_context(LEVEL_GROUP, display_flag=display_flag, verbose=verbose)\n",
    "\n",
    "    df = execute_ctx(init_group_id, df)\n",
    "    df = execute_ctx(deal_unmerged_places, df)\n",
    "    df = execute_ctx(deal_merged_places, df)\n",
    "    \n",
    "    df = execute_ctx(choose_prefered_document, df)\n",
    "    df = execute_ctx(create_agrupamento_inspecao, df)\n",
    "    df = execute_ctx(grouped_subs_asterisk, df)\n",
    "    #df = execute_ctx(final_ajustes, df)\n",
    "    print(f'Main data treat Execution time: {time.time() - start:.2f} seconds')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c98dc4a1-2b0f-4e73-aaf9-d1cf4f9636e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def execute(func, df, display_flag=True, verbose=True, LEVEL_GROUP=None, use_level=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Executes a given function on the DataFrame, optionally displays the result, and shows execution time.\n",
    "\n",
    "    Parameters:\n",
    "    - func: The processing function to execute. It should accept a DataFrame and return a DataFrame.\n",
    "    - df: The DataFrame to process.\n",
    "    - display_flag: Boolean flag indicating whether to display the DataFrame after processing.\n",
    "    - verbose: Boolean flag indicating whether to print execution details.\n",
    "    - LEVEL_GROUP: Optional parameter to pass a level group to the function.\n",
    "    - use_level: Boolean flag indicating whether to use the LEVEL_GROUP parameter.\n",
    "    - **kwargs: Additional keyword arguments to pass to the function.\n",
    "\n",
    "    Returns:\n",
    "    - The processed DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()  # Start timing before function execution\n",
    "  \n",
    "\n",
    "    if verbose:\n",
    "        print(f'_______________\\nExecuting function {func.__name__}...')\n",
    "\n",
    "    if use_level:\n",
    "        df = func(df, LEVEL_GROUP=LEVEL_GROUP, **kwargs)\n",
    "    else:\n",
    "        df = func(df, **kwargs)\n",
    "\n",
    "    end_time = time.time()  # End timing after function execution\n",
    "    execution_time = end_time - start_time  # Calculate execution time\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Function {func.__name__} executed successfully in {execution_time:.2f} seconds.')\n",
    "    \n",
    "    \n",
    "      \n",
    "    if 'group_id' in df.columns:\n",
    "        s = df['group_id'].str.len()\n",
    "        print(f'Result Max group id size: {s.max()}')\n",
    "        \n",
    "        display(df.groupby('group_id').agg({\n",
    "            #'group_id': 'nunique',\n",
    "            'merchant_market_hierarchy_id': 'nunique'\n",
    "        }).max())\n",
    "        \n",
    "        \n",
    "    if 'group_id_index' in df.columns:\n",
    "        sizes = df.groupby('group_id_index').ngroups\n",
    "        print(f'qtd names group id: {sizes}')\n",
    "        \n",
    "        \n",
    "    #if 'group_id' in df.columns: print(f'Number of groups: {df[\"group_id\"].nunique()}')\n",
    "    \n",
    "    if display_flag:\n",
    "        display(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def execute_with_context(LEVEL_GROUP, display_flag=False, verbose=False):\n",
    "    import inspect\n",
    "    def execute_context(func, df, **kwargs):\n",
    "        # Check if 'LEVEL_GROUP' is in the function's parameters\n",
    "        params = inspect.signature(func).parameters\n",
    "        if 'LEVEL_GROUP' in params:\n",
    "            # If 'LEVEL_GROUP' is a parameter, pass it along with 'df'\n",
    "            result = execute(func, df, display_flag=display_flag, verbose=verbose, LEVEL_GROUP=LEVEL_GROUP,\n",
    "                             use_level=True, **kwargs)\n",
    "        else:\n",
    "            # If 'LEVEL_GROUP' is not a parameter, call the function without it\n",
    "            result = execute(func, df, display_flag=display_flag, verbose=verbose, use_level=False, **kwargs)\n",
    "        return result\n",
    "    return execute_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a819d-1cf2-45c1-97fb-9d99a41e2fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.4-2.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-2:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
