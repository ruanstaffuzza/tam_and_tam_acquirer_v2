{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d89bdcd4-5c49-4bf9-8001-8632c531edad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_gbq\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "from projectutils import read_data \n",
    "import sys\n",
    "\n",
    "#import sys, importlib\n",
    "#importlib.reload(sys.modules['group_tam_id'])\n",
    "from group_tam_id import (\n",
    "    load_to_gbq,\n",
    "    main_data_treat_muni,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6165d8d6-d2e3-44b2-8b55-61924ef34ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def vm_load_grouping_part(input_date, cod_munis):\n",
    "    import os\n",
    "    start_time = time.time()\n",
    "    anomes = input_date[:7].replace('-', '')\n",
    "    output_dir = fr'data/temp_dir_{anomes}' \n",
    "    df = pd.concat([pd.read_parquet(f\"{output_dir}/processed_group_{cod_muni}.parquet\") for cod_muni in cod_munis])\n",
    "    print(f'Loading data in {time.time()  - start_time:.2f} seconds.')\n",
    "    df['agrupamento_nome_1'] = df['agrupamento_nome_1'] + df['cod_muni'].astype(str)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df7e9764-2a85-4a95-9f95-253067e7cab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_grouped_part(df, input_date, cod_munis):\n",
    "    df = df.copy()\n",
    "    print('Tamanho do DataFrame:', df.shape)\n",
    "    df = df.merge(vm_load_grouping_part(input_date, cod_munis), on=['inicio', 'cod_muni', 'nome_master'], how='inner')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c12aa6c-fff9-4036-a2e6-8dac872c70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#OUTROS = ['Outros', 'Outros_Pags', 'Outros_SumUp', 'Outros_Stone']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d3acb86-7cad-49be-b219-b2f436e2d684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "import datetime\n",
    "\n",
    "def upload_partitioned_table_to_bigquery(df, destination_table):\n",
    "    \"\"\"\n",
    "    Uploads a DataFrame to BigQuery with partitioning on 'ingestion_date' column.\n",
    "    If the table exists, it appends the data. If not, it creates the table.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to upload.\n",
    "        destination_table (str): Destination table in the format 'project.dataset.table'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Add 'ingestion_date' column\n",
    "    df['ingestion_date'] = datetime.datetime.now()\n",
    "\n",
    "    # Create BigQuery client\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Define job configuration\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        time_partitioning=bigquery.TimePartitioning(\n",
    "            type_=bigquery.TimePartitioningType.DAY,\n",
    "            field=\"ingestion_date\"\n",
    "        ),\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_APPEND  # Append data if table exists, otherwise create table\n",
    "    )\n",
    "\n",
    "    # Load DataFrame to BigQuery\n",
    "    job = client.load_table_from_dataframe(df, destination_table, job_config=job_config)\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    job.result()\n",
    "    print(\"Data successfully loaded into BigQuery\")\n",
    "    \n",
    "    \n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def load_available_data(directory, destination_table, final_data_treat_func, track_file='uploaded_files.json', restart_track_file=True):\n",
    "    \"\"\"\n",
    "    Loads data from parquet files in the given directory and uploads them to a BigQuery table.\n",
    "    Tracks uploaded files to avoid re-uploading.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The directory containing parquet files.\n",
    "        destination_table (str): The BigQuery table to upload data to.\n",
    "        track_file (str): The path to the JSON file used to track uploaded files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the list of uploaded files\n",
    "    if os.path.exists(track_file) and not restart_track_file:\n",
    "        with open(track_file, 'r') as f:\n",
    "            uploaded_files = json.load(f)\n",
    "    else:\n",
    "        uploaded_files = []\n",
    "\n",
    "    # Get list of files in the directory\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('parquet')]\n",
    "\n",
    "    for file in files:\n",
    "        if file not in uploaded_files:\n",
    "            print(f'Opening {file}...')\n",
    "            df = pd.read_parquet(os.path.join(directory, file))\n",
    "            \n",
    "            df = final_data_treat_func(df)\n",
    "            \n",
    "            print(f'Sending {file} to {destination_table}')\n",
    "            upload_partitioned_table_to_bigquery(df, destination_table)\n",
    "            \n",
    "            print(f'{file} sent to {destination_table} successfully!')\n",
    "\n",
    "            # Update the list of uploaded files and save it\n",
    "            uploaded_files.append(file)\n",
    "            with open(track_file, 'w') as f:\n",
    "                json.dump(uploaded_files, f)\n",
    "        else:\n",
    "            print(f'{file} has already been uploaded. Skipping.')\n",
    "\n",
    "\n",
    "def final_data_treat(df):\n",
    "    if '_merge' in df.columns:\n",
    "        df = df.drop(columns=['_merge'])             \n",
    "    else:\n",
    "        print('_merge not in df')\n",
    "    \n",
    "    if 'reference_month_x' in df.columns:\n",
    "        df['reference_month'] = df['reference_month_x']\n",
    "        del df['reference_month_x'], df['reference_month_y']\n",
    "             \n",
    "    #df['group_id_index'] = (df['group_id_index'].astype(str) + df['cod_muni'].astype(str)).astype(int)\n",
    "    \n",
    "    #df = df.drop(columns = ['inicio'])\n",
    "             \n",
    "    return df\n",
    "\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95896bcf-8201-49af-ae76-845f27e0af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inter_data_treat_muni(df, file_idx):\n",
    "\n",
    "    len_df = len(df)\n",
    "    \n",
    "    #if len_df> 100000:\n",
    "    #    verbose = True\n",
    "    #else:\n",
    "    #    verbose = False\n",
    "    verbose = True\n",
    "    LEVEL_GROUP = ['cod_muni']\n",
    "    \n",
    "    cod_munis = list(df['cod_muni'].unique())\n",
    "    \n",
    "    start = time.time()\n",
    "    if len(cod_munis) > 3:\n",
    "        print(f'Starting inter data treat for qtd munis {len(cod_munis)}, with size {len(df)}')\n",
    "    else:\n",
    "        print(f'Starting inter data treat for munis {cod_munis}, with size {len(df)}')\n",
    "        \n",
    "\n",
    "    df = add_grouped_part(df, cod_munis) \n",
    "   \n",
    "    print(f'Inter data treat Execution time file {file_idx}: {time.time() - start:.2f} seconds')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def save_tratados(file_name, input_date):\n",
    "    df = pd.read_parquet(f'data_new/intermediary/{input_date}/{file_name}')\n",
    "    df = main_data_treat_muni(df, file_name)\n",
    "    df.to_parquet(f'data_new/tratados/{input_date}/{file_name}')\n",
    "    print(f'File {file_name} Saved in Tratados')\n",
    "              \n",
    "def save_all_tratados(input_date):\n",
    "    output_dir = f'data_new/intermediary/{input_date}'\n",
    "    \n",
    "    #make sure 'data/intermediary/{input_date}' is created\n",
    "    os.makedirs(f'data_new/tratados/{input_date}', exist_ok=True)\n",
    "    \n",
    "    files = [x for x in os.listdir(output_dir) if x.endswith('parquet')]\n",
    "    for f in files:\n",
    "        save_tratados(f, input_date)\n",
    "\n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f72fb130-aae9-416b-ba7f-16255fb07f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def generate_parquet_summary(directory):\n",
    "    # Lista todos os arquivos no diretório\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.parquet')]\n",
    "    \n",
    "    # Cria listas para armazenar os nomes dos arquivos e a contagem de linhas\n",
    "    file_names = []\n",
    "    line_counts = []\n",
    "    \n",
    "    # Itera sobre cada arquivo, conta as linhas e armazena as informações\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        parquet_file = pq.ParquetFile(file_path)\n",
    "        file_names.append(file)\n",
    "        line_counts.append(parquet_file.metadata.num_rows)\n",
    "    \n",
    "    # Cria o DataFrame\n",
    "    summary_df = pd.DataFrame({\n",
    "        'file_name': file_names,\n",
    "        'line_count': line_counts\n",
    "    })\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "\n",
    "\n",
    "def calculate_group_ids(column):\n",
    "    # Inicializa listas para soma cumulativa e IDs\n",
    "    stopped_cumsum = []\n",
    "    group_ids = []\n",
    "    \n",
    "    # Variáveis auxiliares\n",
    "    cumsum = 0\n",
    "    group_id = 1\n",
    "    \n",
    "    # Itera pelos valores da coluna\n",
    "    for value in column:\n",
    "        if cumsum + value > 1000000:\n",
    "            group_id += 1\n",
    "            cumsum = 0\n",
    "        \n",
    "        cumsum += value\n",
    "        stopped_cumsum.append(cumsum)\n",
    "        group_ids.append(group_id)\n",
    "        \n",
    "        if cumsum == 5:\n",
    "            cumsum = 0\n",
    "            group_id += 1\n",
    "    \n",
    "    return pd.Series(group_ids, index=column.index)\n",
    "\n",
    "\n",
    "def create_agg_nomes_agrupados(input_date):\n",
    "    # Exemplo de uso\n",
    "    anomes = input_date[:7].replace('-', '')\n",
    "    directory = f'data/temp_dir_{anomes}'\n",
    "    df = generate_parquet_summary(directory)\n",
    "    df = df.sort_values('line_count')\n",
    "    df['id'] = calculate_group_ids(df['line_count'])\n",
    "    df['cod_muni'] = df['file_name'].apply(lambda filename: filename.split('_')[2].split('.')[0])\n",
    "\n",
    "    import json\n",
    "    dict_files = df.groupby('id')['cod_muni'].apply(lambda x: list(x)).to_dict()\n",
    "    return dict_files\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76534e98-2687-4793-ba81-0083cc76f35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_gbq as pd_gbq\n",
    "import jinja2\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "# function to get current date and time\n",
    "def get_current_time():\n",
    "    from datetime import datetime\n",
    "    return \"[\" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"]\"\n",
    "\n",
    "def read_gbq_(query):\n",
    "  project_id = 'sfwthr2a4shdyuogrt3jjtygj160rs' # ri-nonprod\n",
    "  print(f'{get_current_time()} Getting dataset from BQ...')\n",
    "  return pd_gbq.read_gbq(query, progress_bar_type='tqdm',\n",
    "                         use_bqstorage_api=True,\n",
    "      project_id=project_id)\n",
    "\n",
    "def read_gbq_from_template(template_query, dict_query):\n",
    "  query = template_query\n",
    "  if dict_query:\n",
    "      from jinja2 import Template\n",
    "      # Reads a query from a template and returns the query with the variables replaced\n",
    "      # template_query: query as string, may use jinja2 templating\n",
    "      # dict_query: dictionary of query parameters, to render from the template with jinja2\n",
    "      query = Template(template_query).render(dict_query)\n",
    "  return read_gbq_(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98c6d8f5-58d4-49a9-b3b0-51d35e46a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_files():\n",
    "    input_date = '2024-04-30'\n",
    "    directory = f'data/intermediary/{input_date}'\n",
    "    files = [x for x in os.listdir(directory) if x.endswith('.parquet')]\n",
    "    for file in files:\n",
    "        os.remove(f'{directory}/{file}')\n",
    "        print(f'File {file} removed from {directory}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f002839-55ce-4d59-aefe-9baea0b53001",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select * from  `dataplatform-prd.master_contact.aux_tam_final_nomes` \n",
    "where 1=1 {{add_filter}}\n",
    "and inicio<>'SYMPLA'\n",
    "\"\"\"\n",
    " \n",
    "def accumulate_and_apply(input_date):\n",
    "    \n",
    "    def save_func(df, idx, cod_munis, input_date):\n",
    "        print(f'Starting inter data treat for file {idx}')\n",
    "        start = time.time()\n",
    "        df = add_grouped_part(df, input_date, cod_munis)\n",
    "        df.to_parquet(f'data_new/intermediary/{input_date}/part_{idx}.parquet')\n",
    "        print(f'Inter data treat Execution time: {time.time() - start:.2f} seconds')\n",
    "        print(f'File part_{idx}.parquet Saved') \n",
    "    \n",
    "    #make sure 'data/intermediary/{input_date}' is created\n",
    "    os.makedirs(f'data_new/intermediary/{input_date}', exist_ok=True)\n",
    "    \n",
    "    \"\"\"concat files in 'nomes_agrupados', based in 'dict_files' and apply function\"\"\"\n",
    "\n",
    "    dict_files = create_agg_nomes_agrupados(input_date)\n",
    "    dict_files = {k: [int(x) for x in v] for k, v in dict_files.items()}\n",
    "\n",
    "    df = read_gbq_from_template(query, {'add_filter': f'AND reference_month = \"{input_date}\"'})\n",
    "    print(f'File Read: {input_date}')\n",
    "    [save_func(df[df['cod_muni'].isin(v)], k, v, input_date) for k, v in dict_files.items()]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15a70e28-3a65-4416-9d77-c9b9d9da3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_str = '2024-10-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eec5adac-be17-45dd-960b-024885dcb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from main_agrupamento_nomes import main_agrupamento_nomes\n",
    "#main_agrupamento_nomes(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1195feb-5297-4ab5-99aa-98705d648279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-27 22:37:57] Getting dataset from BQ...\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      "File Read: 2024-10-31\n",
      "Starting inter data treat for file 1\n",
      "Tamanho do DataFrame: (1095591, 13)\n",
      "Loading data in 11.84 seconds.\n",
      "Inter data treat Execution time: 20.78 seconds\n",
      "File part_1.parquet Saved\n",
      "Starting inter data treat for file 2\n",
      "Tamanho do DataFrame: (1102404, 13)\n",
      "Loading data in 4.55 seconds.\n",
      "Inter data treat Execution time: 12.90 seconds\n",
      "File part_2.parquet Saved\n",
      "Starting inter data treat for file 3\n",
      "Tamanho do DataFrame: (1109117, 13)\n",
      "Loading data in 2.84 seconds.\n",
      "Inter data treat Execution time: 10.84 seconds\n",
      "File part_3.parquet Saved\n",
      "Starting inter data treat for file 4\n",
      "Tamanho do DataFrame: (1113670, 13)\n",
      "Loading data in 2.13 seconds.\n",
      "Inter data treat Execution time: 10.19 seconds\n",
      "File part_4.parquet Saved\n",
      "Starting inter data treat for file 5\n",
      "Tamanho do DataFrame: (1119053, 13)\n",
      "Loading data in 1.92 seconds.\n",
      "Inter data treat Execution time: 9.78 seconds\n",
      "File part_5.parquet Saved\n",
      "Starting inter data treat for file 6\n",
      "Tamanho do DataFrame: (1125713, 13)\n",
      "Loading data in 1.73 seconds.\n",
      "Inter data treat Execution time: 9.76 seconds\n",
      "File part_6.parquet Saved\n",
      "Starting inter data treat for file 7\n",
      "Tamanho do DataFrame: (1120246, 13)\n",
      "Loading data in 1.55 seconds.\n",
      "Inter data treat Execution time: 9.73 seconds\n",
      "File part_7.parquet Saved\n",
      "Starting inter data treat for file 8\n",
      "Tamanho do DataFrame: (1136164, 13)\n",
      "Loading data in 1.71 seconds.\n",
      "Inter data treat Execution time: 10.08 seconds\n",
      "File part_8.parquet Saved\n",
      "Starting inter data treat for file 9\n",
      "Tamanho do DataFrame: (1122320, 13)\n",
      "Loading data in 1.54 seconds.\n",
      "Inter data treat Execution time: 9.79 seconds\n",
      "File part_9.parquet Saved\n",
      "Starting inter data treat for file 10\n",
      "Tamanho do DataFrame: (1132607, 13)\n",
      "Loading data in 1.46 seconds.\n",
      "Inter data treat Execution time: 9.69 seconds\n",
      "File part_10.parquet Saved\n",
      "Starting inter data treat for file 11\n",
      "Tamanho do DataFrame: (1144534, 13)\n",
      "Loading data in 1.44 seconds.\n",
      "Inter data treat Execution time: 9.57 seconds\n",
      "File part_11.parquet Saved\n",
      "Starting inter data treat for file 12\n",
      "Tamanho do DataFrame: (1119729, 13)\n",
      "Loading data in 1.48 seconds.\n",
      "Inter data treat Execution time: 9.57 seconds\n",
      "File part_12.parquet Saved\n",
      "Starting inter data treat for file 13\n",
      "Tamanho do DataFrame: (1141112, 13)\n",
      "Loading data in 1.68 seconds.\n",
      "Inter data treat Execution time: 9.71 seconds\n",
      "File part_13.parquet Saved\n",
      "Starting inter data treat for file 14\n",
      "Tamanho do DataFrame: (1101934, 13)\n",
      "Loading data in 1.68 seconds.\n",
      "Inter data treat Execution time: 8.92 seconds\n",
      "File part_14.parquet Saved\n",
      "Starting inter data treat for file 15\n",
      "Tamanho do DataFrame: (1136507, 13)\n",
      "Loading data in 1.59 seconds.\n",
      "Inter data treat Execution time: 9.07 seconds\n",
      "File part_15.parquet Saved\n",
      "Starting inter data treat for file 16\n",
      "Tamanho do DataFrame: (934487, 13)\n",
      "Loading data in 1.39 seconds.\n",
      "Inter data treat Execution time: 7.38 seconds\n",
      "File part_16.parquet Saved\n",
      "Starting inter data treat for file 17\n",
      "Tamanho do DataFrame: (743224, 13)\n",
      "Loading data in 1.03 seconds.\n",
      "Inter data treat Execution time: 5.80 seconds\n",
      "File part_17.parquet Saved\n",
      "Starting inter data treat for file 18\n",
      "Tamanho do DataFrame: (495251, 13)\n",
      "Loading data in 0.67 seconds.\n",
      "Inter data treat Execution time: 3.66 seconds\n",
      "File part_18.parquet Saved\n",
      "Starting inter data treat for file 19\n",
      "Tamanho do DataFrame: (776803, 13)\n",
      "Loading data in 0.85 seconds.\n",
      "Inter data treat Execution time: 5.70 seconds\n",
      "File part_19.parquet Saved\n",
      "Starting inter data treat for file 20\n",
      "Tamanho do DataFrame: (1843608, 13)\n",
      "Loading data in 3.51 seconds.\n",
      "Inter data treat Execution time: 16.16 seconds\n",
      "File part_20.parquet Saved\n"
     ]
    }
   ],
   "source": [
    "accumulate_and_apply(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a02763e-d9f1-4fa5-924b-0ae93678781d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-31\n",
      "Starting main data treat for file part_7.parquet, with size 1120246\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 1.27 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 18.00 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 23.14 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 11.13 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 9.96 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 11.37 seconds.\n",
      "Main data treat Execution time file part_7.parquet: 75.27 seconds\n",
      "File part_7.parquet Saved in Tratados\n",
      "Starting main data treat for file part_10.parquet, with size 1132607\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 1.29 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 18.19 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 23.19 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 11.27 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 10.01 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 9.65 seconds.\n",
      "Main data treat Execution time file part_10.parquet: 74.01 seconds\n",
      "File part_10.parquet Saved in Tratados\n",
      "Starting main data treat for file part_19.parquet, with size 776803\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 0.81 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 11.76 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 15.06 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 7.40 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 6.42 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 2.63 seconds.\n",
      "Main data treat Execution time file part_19.parquet: 44.32 seconds\n",
      "File part_19.parquet Saved in Tratados\n",
      "Starting main data treat for file part_6.parquet, with size 1125713\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 1.26 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 18.11 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 23.30 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 11.12 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 9.95 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 11.78 seconds.\n",
      "Main data treat Execution time file part_6.parquet: 75.94 seconds\n",
      "File part_6.parquet Saved in Tratados\n",
      "Starting main data treat for file part_9.parquet, with size 1122320\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 1.29 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 18.04 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 22.82 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 11.14 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 9.86 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 10.08 seconds.\n",
      "Main data treat Execution time file part_9.parquet: 73.64 seconds\n",
      "File part_9.parquet Saved in Tratados\n",
      "Starting main data treat for file part_11.parquet, with size 1144534\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 1.22 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 18.50 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 23.69 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 11.25 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 9.90 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 9.21 seconds.\n",
      "Main data treat Execution time file part_11.parquet: 74.17 seconds\n",
      "File part_11.parquet Saved in Tratados\n",
      "Starting main data treat for file part_17.parquet, with size 743224\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 0.77 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 11.37 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 14.07 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 7.10 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 6.39 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 4.12 seconds.\n",
      "Main data treat Execution time file part_17.parquet: 44.06 seconds\n",
      "File part_17.parquet Saved in Tratados\n",
      "Starting main data treat for file part_12.parquet, with size 1119729\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 1.24 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 17.88 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 22.78 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 10.95 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 9.61 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 8.83 seconds.\n",
      "Main data treat Execution time file part_12.parquet: 71.68 seconds\n",
      "File part_12.parquet Saved in Tratados\n",
      "Starting main data treat for file part_1.parquet, with size 1095591\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 1.23 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 17.65 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 22.58 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 10.72 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 9.61 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 21.52 seconds.\n",
      "Main data treat Execution time file part_1.parquet: 83.71 seconds\n",
      "File part_1.parquet Saved in Tratados\n",
      "Starting main data treat for file part_3.parquet, with size 1109117\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 1.29 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 17.93 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 22.89 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 10.86 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 9.93 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 13.07 seconds.\n",
      "Main data treat Execution time file part_3.parquet: 76.37 seconds\n",
      "File part_3.parquet Saved in Tratados\n",
      "Starting main data treat for file part_2.parquet, with size 1102404\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 1.22 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 17.77 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 22.74 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 10.78 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 9.87 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 16.72 seconds.\n",
      "Main data treat Execution time file part_2.parquet: 79.52 seconds\n",
      "File part_2.parquet Saved in Tratados\n",
      "Starting main data treat for file part_15.parquet, with size 1136507\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 1.25 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 17.96 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 22.83 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 11.07 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 10.02 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 7.10 seconds.\n",
      "Main data treat Execution time file part_15.parquet: 70.63 seconds\n",
      "File part_15.parquet Saved in Tratados\n",
      "Starting main data treat for file part_4.parquet, with size 1113670\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 1.25 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 17.98 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 22.88 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 10.94 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 10.01 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 14.23 seconds.\n",
      "Main data treat Execution time file part_4.parquet: 77.70 seconds\n",
      "File part_4.parquet Saved in Tratados\n",
      "Starting main data treat for file part_5.parquet, with size 1119053\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 1.27 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 17.98 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 22.77 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 10.99 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 10.04 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 11.18 seconds.\n",
      "Main data treat Execution time file part_5.parquet: 74.64 seconds\n",
      "File part_5.parquet Saved in Tratados\n",
      "Starting main data treat for file part_20.parquet, with size 1843608\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 1.91 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 29.04 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 36.14 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 17.59 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 15.06 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 6.16 seconds.\n",
      "Main data treat Execution time file part_20.parquet: 106.47 seconds\n",
      "File part_20.parquet Saved in Tratados\n",
      "Starting main data treat for file part_18.parquet, with size 495251\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 0.40 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 7.65 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 10.37 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 4.24 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 3.60 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 1.57 seconds.\n",
      "Main data treat Execution time file part_18.parquet: 27.97 seconds\n",
      "File part_18.parquet Saved in Tratados\n",
      "Starting main data treat for file part_8.parquet, with size 1136164\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 1.29 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 18.38 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 23.48 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 11.28 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 10.10 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 11.59 seconds.\n",
      "Main data treat Execution time file part_8.parquet: 76.54 seconds\n",
      "File part_8.parquet Saved in Tratados\n",
      "Starting main data treat for file part_14.parquet, with size 1101934\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 1.28 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 18.00 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 22.66 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 11.19 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 10.36 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 7.64 seconds.\n",
      "Main data treat Execution time file part_14.parquet: 71.53 seconds\n",
      "File part_14.parquet Saved in Tratados\n",
      "Starting main data treat for file part_13.parquet, with size 1141112\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 1.34 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 18.80 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 23.58 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 11.14 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 10.44 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 9.41 seconds.\n",
      "Main data treat Execution time file part_13.parquet: 75.12 seconds\n",
      "File part_13.parquet Saved in Tratados\n",
      "Starting main data treat for file part_16.parquet, with size 934487\n",
      "Executing function prepare_data...\n",
      "Function prepare_data executed successfully in 1.15 seconds.\n",
      "Executing function init_group_id...\n",
      "Function init_group_id executed successfully in 14.99 seconds.\n",
      "Executing function deal_unmerged_places...\n",
      "Function deal_unmerged_places executed successfully in 18.93 seconds.\n",
      "Executing function deal_merged_places...\n",
      "Function deal_merged_places executed successfully in 9.44 seconds.\n",
      "Executing function deal_merged_docs...\n",
      "Function deal_merged_docs executed successfully in 8.45 seconds.\n",
      "Executing function pos_tratamento...\n",
      "Function pos_tratamento executed successfully in 5.63 seconds.\n",
      "Main data treat Execution time file part_16.parquet: 58.92 seconds\n",
      "File part_16.parquet Saved in Tratados\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(date_str)\n",
    "save_all_tratados(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c8bfd4f-04ea-4216-92ec-d8eff9ad73d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening part_7.parquet...\n",
      "_merge not in df\n",
      "Sending part_7.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_7.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_10.parquet...\n",
      "_merge not in df\n",
      "Sending part_10.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_10.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_19.parquet...\n",
      "_merge not in df\n",
      "Sending part_19.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_19.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_6.parquet...\n",
      "_merge not in df\n",
      "Sending part_6.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_6.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_9.parquet...\n",
      "_merge not in df\n",
      "Sending part_9.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_9.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_11.parquet...\n",
      "_merge not in df\n",
      "Sending part_11.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_11.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_17.parquet...\n",
      "_merge not in df\n",
      "Sending part_17.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_17.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_12.parquet...\n",
      "_merge not in df\n",
      "Sending part_12.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_12.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_1.parquet...\n",
      "_merge not in df\n",
      "Sending part_1.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_1.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_3.parquet...\n",
      "_merge not in df\n",
      "Sending part_3.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_3.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_2.parquet...\n",
      "_merge not in df\n",
      "Sending part_2.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_2.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_15.parquet...\n",
      "_merge not in df\n",
      "Sending part_15.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_15.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_4.parquet...\n",
      "_merge not in df\n",
      "Sending part_4.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_4.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_5.parquet...\n",
      "_merge not in df\n",
      "Sending part_5.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_5.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_20.parquet...\n",
      "_merge not in df\n",
      "Sending part_20.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_20.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_18.parquet...\n",
      "_merge not in df\n",
      "Sending part_18.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_18.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_8.parquet...\n",
      "_merge not in df\n",
      "Sending part_8.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_8.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_14.parquet...\n",
      "_merge not in df\n",
      "Sending part_14.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_14.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_13.parquet...\n",
      "_merge not in df\n",
      "Sending part_13.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_13.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n",
      "Opening part_16.parquet...\n",
      "_merge not in df\n",
      "Sending part_16.parquet to dataplatform-prd.master_contact.aux_tam_python_agrupados\n",
      "Data successfully loaded into BigQuery\n",
      "part_16.parquet sent to dataplatform-prd.master_contact.aux_tam_python_agrupados successfully!\n"
     ]
    }
   ],
   "source": [
    "load_available_data(directory=f'data_new/tratados/{date_str}', final_data_treat_func=final_data_treat, destination_table='dataplatform-prd.master_contact.aux_tam_python_agrupados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8e5ffce-d118-4a1e-832c-719d49289bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fim Upload\n"
     ]
    }
   ],
   "source": [
    "print('Fim Upload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f607ed80-00b7-4250-a3b3-2155e9fbe793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-10-31'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8c0f8-92d6-4569-8cf4-18c338e6d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(date):\n",
    "    accumulate_and_apply(date)\n",
    "    save_all_tratados(date)\n",
    "    load_available_data(directory=f'data_new/tratados/{date}', final_data_treat_func=final_data_treat, destination_table='dataplatform-prd.master_contact.aux_tam_python_agrupados')\n",
    "    print('Fim Upload', date)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ff82de-f99f-4af1-9ef4-d032fdcfdc15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.4-2.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-2:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
